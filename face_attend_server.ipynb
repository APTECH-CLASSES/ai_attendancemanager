{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66117993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify  # Core Flask tools\n",
    "from flask_cors import CORS  # Enable cross-origin for React\n",
    "import cv2  # Image decoding\n",
    "import numpy as np  # Array ops\n",
    "import pickle  # Load model\n",
    "import sqlite3  # Local database\n",
    "from datetime import datetime  # Time stamping\n",
    "import logging  # Logging\n",
    "from skimage.feature import hog  # HOG extraction\n",
    "import os  # Directory checks\n",
    "from sklearn.svm import SVC  # SVM refit\n",
    "from sklearn.preprocessing import StandardScaler  # Scaling\n",
    "from sklearn.model_selection import train_test_split  # Retrain split\n",
    "from sklearn.metrics import accuracy_score  # Retrain eval\n",
    "\n",
    "app = Flask(__name__)  # Init app\n",
    "CORS(app, origins=[\"http://localhost:5173\"])  # Unique: Specific origin for Vite\n",
    "\n",
    "# Sub-action 1: Log setup\n",
    "log_path = \"activity_logs\"  # Reuse directory\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)\n",
    "server_log = f'{log_path}/server_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "logging.basicConfig(filename=server_log, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Sub-action 2: Model load\n",
    "data_store = \"ready_face_data\"  # Data path\n",
    "model_path = \"svm_face_model.pkl\"  # Model file\n",
    "if os.path.exists(model_path):\n",
    "    with open(model_path, 'rb') as model_handle:  # Sub-action: Deserialize\n",
    "        loaded_pack = pickle.load(model_handle)\n",
    "        face_recognizer = loaded_pack['svm_model']  # Extract classifier\n",
    "        feat_normalizer = loaded_pack['feature_scaler']  # Extract scaler\n",
    "        label_key = loaded_pack['tag_dict']  # Extract map\n",
    "    logging.info(\"Model loaded from file.\")\n",
    "else:\n",
    "    logging.error(\"Model file missing - run Phase 2.\")\n",
    "    face_recognizer, feat_normalizer, label_key = None, None, {}\n",
    "\n",
    "def get_hog_desc(gray_crop):  # Renamed, tweaked params\n",
    "    \"\"\"Sub-action: Generate HOG from grayscale crop.\"\"\"\n",
    "    desc_vector, _ = hog(gray_crop, orientations=8, pixels_per_cell=(10, 10),  # Unique params\n",
    "                         cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return desc_vector  # Return descriptor\n",
    "\n",
    "face_detect = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')  # Load cascade\n",
    "\n",
    "def find_valid_face(input_frame):  # Renamed\n",
    "    \"\"\"Main action: Detect one face, convert/resize to gray.\n",
    "    Sub-action: Convert to gray.\n",
    "    Testable: Log detection count.\n",
    "    \"\"\"\n",
    "    gray_input = cv2.cvtColor(input_frame, cv2.COLOR_BGR2GRAY)  # Sub-action: To grayscale\n",
    "    detected = face_detect.detectMultiScale(gray_input, 1.3, 5)  # Sub-action: Detect\n",
    "    if len(detected) == 1:  # Sub-action: Ensure single face (anti-proxy)\n",
    "        x_coord, y_coord, width, height = detected[0]\n",
    "        face_crop = gray_input[y_coord:y_coord+height, x_coord:x_coord+width]  # Crop\n",
    "        resized_crop = cv2.resize(face_crop, (100, 100), interpolation=cv2.INTER_LINEAR)  # Resize\n",
    "        unique_vals = np.unique(resized_crop)  # Sub-action: Grayscale check\n",
    "        if len(unique_vals) <= 2:  # Basic gray validation\n",
    "            logging.info(\"Valid single grayscale face detected.\")  # Log success\n",
    "            return resized_crop\n",
    "    logging.warning(f\"Invalid detection: {len(detected)} faces\")  # Log issue\n",
    "    return None  # No valid face\n",
    "\n",
    "def get_db_connection():  # Unique function for DB\n",
    "    \"\"\"Sub-action: Establish SQLite connection.\"\"\"\n",
    "    return sqlite3.connect('attendance.db')\n",
    "\n",
    "@app.route('/process_entry', methods=['POST'])  # Unique endpoint\n",
    "def process_entry():\n",
    "    \"\"\"Main action: Handle attendance scan.\n",
    "    Sub-action: Validate image file.\n",
    "    Testable: Log confidence.\n",
    "    \"\"\"\n",
    "    if 'face_capture' not in request.files:  # Sub-action: Check file\n",
    "        logging.error(\"Missing face image in request.\")\n",
    "        return jsonify({'error': 'No image provided'}), 400\n",
    "\n",
    "    file_handle = request.files['face_capture']  # Get file\n",
    "    buffer_array = np.frombuffer(file_handle.read(), np.uint8)  # Sub-action: Buffer\n",
    "    decoded_frame = cv2.imdecode(buffer_array, cv2.IMREAD_COLOR)  # Decode\n",
    "\n",
    "    valid_crop = find_valid_face(decoded_frame)  # Sub-action: Detect face\n",
    "    if valid_crop is None:\n",
    "        return jsonify({'status': 'absent', 'message': 'No valid grayscale face'})\n",
    "\n",
    "    crop_features = get_hog_desc(valid_crop)  # Sub-action: Extract HOG\n",
    "    normalized_crop = feat_normalizer.transform([crop_features])  # Normalize\n",
    "    prob_scores = face_recognizer.predict_proba(normalized_crop)[0]  # Sub-action: Predict probs\n",
    "    top_idx = np.argmax(prob_scores)  # Get top class\n",
    "    match_conf = prob_scores[top_idx] * 100  # Confidence calc\n",
    "    logging.info(f\"Entry scan confidence: {match_conf:.2f}%\")  # Test log\n",
    "\n",
    "    if match_conf < 95:  # Sub-action: Threshold check (SRS req)\n",
    "        return jsonify({'status': 'absent', 'message': 'Confidence too low'})\n",
    "\n",
    "    matched_tag = list(label_key.keys())[top_idx]  # Get tag\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT id FROM student_profiles WHERE full_name = ?', (matched_tag,))\n",
    "    profile_result = cursor.fetchone()  # Sub-action: Fetch profile\n",
    "    conn.close()\n",
    "\n",
    "    if not profile_result:\n",
    "        return jsonify({'status': 'unknown'})\n",
    "\n",
    "    user_id = profile_result[0]  # Extract ID\n",
    "    current_moment = datetime.now()  # Sub-action: Get time\n",
    "    entry_stamp = current_moment.strftime(\"%H:%M:%S\")  # Format time\n",
    "    day_key = current_moment.strftime(\"%Y-%m-%d\")  # Day for DB\n",
    "\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT present FROM daily_attendance WHERE date = ? AND id = ?', (day_key, user_id))\n",
    "    existing_record = cursor.fetchone()  # Sub-action: Check duplicate\n",
    "    if existing_record and existing_record[0]:\n",
    "        conn.close()\n",
    "        return jsonify({'status': 'already_marked'})\n",
    "\n",
    "    cursor.execute('INSERT OR REPLACE INTO daily_attendance (date, id, present, in_time, out_time, duration) VALUES (?, ?, ?, ?, ?, ?)',\n",
    "                   (day_key, user_id, 1, entry_stamp, None, 0))  # Sub-action: Save record\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    logging.info(f\"Entry marked for {matched_tag} at {entry_stamp}\")  # Log mark\n",
    "\n",
    "    # Sub-action: Check for alerts (SRS req)\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT COUNT(*) FROM daily_attendance WHERE id = ? AND present = 0', (user_id,))\n",
    "    absent_count = cursor.fetchone()[0]\n",
    "    conn.close()\n",
    "    if absent_count >= 3:\n",
    "        logging.warning(f\"Alert: High absences for {matched_tag}\")  # Alert log\n",
    "\n",
    "    return jsonify({'status': 'present', 'name': matched_tag, 'id': user_id, 'in_time': entry_stamp, 'confidence': f\"{match_conf:.2f}\"})\n",
    "\n",
    "@app.route('/handle_checkout', methods=['POST'])  # Unique name\n",
    "def handle_checkout():\n",
    "    \"\"\"Main action: Log out time and duration.\n",
    "    Sub-action: Validate JSON data.\n",
    "    Testable: Log duration.\n",
    "    \"\"\"\n",
    "    checkout_data = request.json  # Get JSON\n",
    "    if not checkout_data or 'id' not in checkout_data:  # Sub-action: Validate\n",
    "        logging.error(\"Invalid checkout data.\")\n",
    "        return jsonify({'error': 'Invalid data'}), 400\n",
    "\n",
    "    user_id = checkout_data['id']  # Extract ID\n",
    "    now_time = datetime.now()  # Get current time\n",
    "    exit_stamp = now_time.strftime(\"%H:%M:%S\")  # Format\n",
    "    day_key = now_time.strftime(\"%Y-%m-%d\")  # Day key\n",
    "\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT in_time FROM daily_attendance WHERE date = ? AND id = ?', (day_key, user_id))\n",
    "    record_dict = cursor.fetchone()  # Fetch record\n",
    "\n",
    "    if record_dict and not cursor.execute('SELECT out_time FROM daily_attendance WHERE date = ? AND id = ?', (day_key, user_id)).fetchone()[0]:  # Sub-action: Check open entry\n",
    "        entry_time_obj = datetime.strptime(record_dict[0], \"%H:%M:%S\")  # Parse in\n",
    "        exit_time_obj = datetime.strptime(exit_stamp, \"%H:%M:%S\")  # Parse out\n",
    "        time_diff = int((exit_time_obj - entry_time_obj).total_seconds() / 60)  # Calc mins\n",
    "        cursor.execute('UPDATE daily_attendance SET out_time = ?, duration = ? WHERE date = ? AND id = ?',\n",
    "                       (exit_stamp, time_diff, day_key, user_id))  # Update\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        logging.info(f\"Checkout for {user_id}: {time_diff} minutes\")  # Test log\n",
    "        return jsonify({'message': 'Checkout complete', 'duration': time_diff})\n",
    "    logging.warning(f\"No open entry for {user_id}\")\n",
    "    conn.close()\n",
    "    return jsonify({'error': 'No open entry'})\n",
    "\n",
    "@app.route('/retrieve_daily_report/<report_date>', methods=['GET'])  # Unique param\n",
    "def retrieve_daily_report(report_date):\n",
    "    \"\"\"Main action: Fetch and analyze daily records.\n",
    "    Sub-action: Query records.\n",
    "    Testable: Log rate.\n",
    "    \"\"\"\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT id, present, in_time, out_time, duration FROM daily_attendance WHERE date = ?', (report_date,))\n",
    "    daily_list = [{'id': row[0], 'present': bool(row[1]), 'in_time': row[2], 'out_time': row[3], 'duration': row[4]} for row in cursor.fetchall()]  # Collect\n",
    "    conn.close()\n",
    "    total_items = len(daily_list)  # Count total\n",
    "    present_items = sum(1 for item in daily_list if item['present'])  # Count present\n",
    "    presence_ratio = (present_items / total_items * 100) if total_items > 0 else 0  # Calc rate\n",
    "    logging.info(f\"Report {report_date}: {present_items}/{total_items} present ({presence_ratio:.1f}%)\")  # Test log\n",
    "    return jsonify({'date': report_date, 'records': daily_list, 'present_rate': f\"{presence_ratio:.1f}%\"})\n",
    "\n",
    "@app.route('/add_student_profile', methods=['POST'])  # Unique endpoint\n",
    "def add_student_profile():\n",
    "    \"\"\"Main action: Enroll new student with 100 images, retrain.\n",
    "    Sub-action: Validate form data.\n",
    "    Testable: Log img count, accuracy.\n",
    "    \"\"\"\n",
    "    if 'new_photos' not in request.files or 'profile_info' not in request.form:  # Sub-action: Check files/form\n",
    "        logging.error(\"Missing photos or info in enrollment.\")\n",
    "        return jsonify({'error': 'Missing data'}), 400\n",
    "\n",
    "    info_str = request.form['profile_info']  # Get info\n",
    "    profile_dict = json.loads(info_str)  # Parse JSON\n",
    "    student_name = profile_dict.get('full_name', '').strip()  # Extract name\n",
    "    student_code = profile_dict.get('id', '').strip()  # Extract code\n",
    "    student_gender = profile_dict.get('gender', 'Unknown')  # Extract gender\n",
    "\n",
    "    if not student_name or not student_code:  # Sub-action: Validate details\n",
    "        logging.error(\"Incomplete student details.\")\n",
    "        return jsonify({'error': 'Incomplete details'}), 400\n",
    "\n",
    "    new_subfolder = f\"{student_name.replace(' ', '_')}_{student_code}\"  # Unique folder name\n",
    "    new_full_path = os.path.join(data_store, new_subfolder)  # Path\n",
    "    os.makedirs(new_full_path, exist_ok=True)  # Create dir\n",
    "    photo_counter = 0  # Counter\n",
    "\n",
    "    for photo in request.files.getlist('new_photos'):  # Sub-action: Loop photos\n",
    "        if photo.filename.lower().endswith(('.jpg', '.jpeg')):  # Filter\n",
    "            buffer_data = np.frombuffer(photo.read(), np.uint8)  # Buffer\n",
    "            decoded_photo = cv2.imdecode(buffer_data, cv2.IMREAD_COLOR)  # Decode\n",
    "            valid_photo_crop = find_valid_face(decoded_photo)  # Detect/validate\n",
    "            if valid_photo_crop is not None:\n",
    "                saved_photo_name = os.path.join(new_full_path, f\"enrolled_gray_{new_subfolder}_{photo_counter+1}.jpg\")  # Unique name\n",
    "                cv2.imwrite(saved_photo_name, valid_photo_crop)  # Save\n",
    "                photo_counter += 1\n",
    "                logging.info(f\"Enrolled photo {photo_counter} for {student_name}\")  # Log\n",
    "\n",
    "    if photo_counter < 100:  # Sub-action: Count check\n",
    "        logging.error(f\"Insufficient photos {photo_counter} for {student_name}.\")\n",
    "        return jsonify({'error': f'Only {photo_counter} photos; require 100'}), 400\n",
    "\n",
    "    # Sub-action: Save to SQLite\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('INSERT OR REPLACE INTO student_profiles (id, full_name, gender, face_folder) VALUES (?, ?, ?, ?)',\n",
    "                   (student_code, student_name, student_gender, new_subfolder))  # Set doc\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    logging.info(f\"Profile saved for {student_name} with {photo_counter} photos\")\n",
    "\n",
    "    # Sub-action: Retrain model\n",
    "    def refresh_model(dataset_path):  # Unique inner func\n",
    "        feat_list, tag_list = gather_dataset_features(dataset_path, (100, 100))  # Reload data\n",
    "        if len(feat_list) == 0:  # Check empty\n",
    "            logging.error(\"No data for refresh.\")\n",
    "            raise ValueError(\"No data for refresh.\")\n",
    "        feat_normalizer.fit(feat_list)  # Refit scaler\n",
    "        scaled_list = feat_normalizer.transform(feat_list)  # Scale\n",
    "        train_split, test_split, train_tag, test_tag = train_test_split(scaled_list, tag_list, test_size=0.2, random_state=202)  # Split\n",
    "        face_recognizer.fit(train_split, train_tag)  # Refit SVM\n",
    "        pred_tag = face_recognizer.predict(test_split)  # Predict\n",
    "        refresh_acc = accuracy_score(test_tag, pred_tag)  # Score\n",
    "        logging.info(f\"Refreshed accuracy: {refresh_acc * 100:.2f}%\")  # Test log\n",
    "        assert refresh_acc >= 0.95, \"Refreshed accuracy below threshold\"  # Test\n",
    "        print(f\"Test: Retrain accuracy = {refresh_acc * 100:.2f}%\")  # Test print\n",
    "        label_key.update({t: i for i, t in enumerate(np.unique(tag_list), len(label_key))})  # Update map\n",
    "        refresh_pack = {'svm_model': face_recognizer, 'feature_scaler': feat_normalizer, 'tag_dict': label_key}  # Pack\n",
    "        with open(model_path, 'wb') as refresh_handle:  # Save\n",
    "            pickle.dump(refresh_pack, refresh_handle)\n",
    "        logging.info(\"Model refreshed and saved.\")\n",
    "        return refresh_pack\n",
    "\n",
    "    refresh_model(data_store)  # Call retrain\n",
    "    return jsonify({'status': 'enrolled', 'message': f'{student_name} added with {photo_counter} photos'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.info(\"Server starting on port 5000.\")  # Startup log\n",
    "    app.run(debug=True, port=5000, host='0.0.0.0')  # Run with host for accessibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
